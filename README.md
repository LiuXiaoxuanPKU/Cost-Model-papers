## Open Source Projects
- Cost Model
  - [nn-Meter: Towards Accurate Latency Prediction of Deep-Learning Model Inference on Diverse Edge Devices](https://github.com/microsoft/nn-Meter)
  - [Paleo: A Performance Model for Deep Neural Networks](https://github.com/TalwalkarLab/paleo)
- Distributed Training
  - [ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning](https://github.com/microsoft/DeepSpeed)
  - [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://github.com/NVIDIA/Megatron-LM)
  - [Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning](https://github.com/alpa-projects)

## Memory Cost Model
- [Estimating GPU Memory Consumption of Deep Learning Models](https://www.microsoft.com/en-us/research/uploads/prod/2020/09/dnnmem.pdf) by Yanjie Gao et al., ESEC/FSE 2020

## Computation Cost Model
- Cost Model for NAS/Cloud
  - [Habitat: A Runtime-Based Computational Performance Predictor for Deep Neural Network Training](https://www.usenix.org/system/files/atc21-yu.pdf) by Geoffrey X. Yu et al., USENIX ATC 2021
  - [To bridge neural network design and real-world performance: A behaviour study for neural networks](https://proceedings.mlsys.org/paper/2021/file/02522a2b2726fb0a03bb19f2d8d9524d-Paper.pdf) by Xiaohu Tang et al., MLSys 2021
  - [perf4sight: A toolflow to model CNN training performance on Edge GPUs](https://arxiv.org/pdf/2108.05580.pdf) by Aditya Rajagopal et al., ArXiv 2021
  - [nn-Meter: Towards Accurate Latency Prediction of Deep-Learning Model Inference on Diverse Edge Devices](https://dl.acm.org/doi/pdf/10.1145/3458864.3467882?casa_token=x0qNEhcP_wAAAAAA:uCTMD3yLynIaS7PwFvxzT65oxmrKz6EyOClSjYNCr-t036yn8VsqJcNjygQDkhR_04NeyZvRWS0e) by Li Lyna Zhang et al., MobiSys 2021
  - [Empirical Analysis and Modeling of Compute Times of CNN Operations on AWS Cloud](https://ieeexplore.ieee.org/abstract/document/9251263) by Ubaid Ullah Hafeez et al., IISWC 2020
  - [Paleo: A Performance Model for Deep Neural Networks](https://openreview.net/pdf?id=SyVVJ85lg) by Hang Qi et al., ICLR 2017
  - [Augur: Modeling the Resource Requirements of Convolutional Neural Networks on Mobile Devices](https://arxiv.org/pdf/1709.09503.pdf) by Zongqing Lu et al., Proceedings of the 25th ACM international conference on Multimedia 2017
  - [Performance Modelling of Deep Learning on Intel Many Integrated Core Architectures](https://arxiv.org/pdf/1906.01992.pdf) by Andre Viebke et al., ArXiv 2019
 
- Cost model for kernel compilation
  - [A learned Performance Model for Tensor Processing Units](https://arxiv.org/abs/2008.01040) by Samuel J. Kaufman et al., MLSys 2021

## Communication Cost Model
- [Iteration Time Prediction for CNN in Multi-GPU Platform: Modeling and Analysis](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8713989) by Ziqian Pei et al., IEEE Access 2019

## Distributed Training
- [Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning](https://arxiv.org/pdf/2201.12023.pdf) by Lianmin Zheng et al., ArXiv 2022
- [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/pdf/1909.08053.pdf) by Mohammad Shoeybi et al., ArXiv 2019
- [ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning](https://arxiv.org/pdf/2104.07857.pdf) by Samyam Rajbhandari et al., SC 2021
- [Improving the Accuracy, Scalability, and Performance of Graph Neural Networks with Roc](https://cs.stanford.edu/~zhihao/papers/mlsys20.pdf) by Zhihao Jia et al., MLSys 2020
- [A Distributed Multi-GPU System for Fast Graph Processing](http://www.vldb.org/pvldb/vol11/p297-jia.pdf) by Zhihao Jia et al., VLDB 2017


## Framework Introduction
- [PyTorch Internal](http://blog.ezyang.com/2019/05/pytorch-internals/)
